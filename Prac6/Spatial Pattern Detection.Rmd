---
title: "Prac6 Detecting Spatial Patterns"
author: "dnkimani"
date: "2024-11-12"
output: html_document
---
# Detecting Spatial Patterns
## 1 Setting up the Data
```{r Packages}
# loading packages
#first library a few packages that we will use during the practical
#note you may need to install them first...
library(tidyverse)
library(spatstat)
library(here)
library(sp)
library(tmap)
library(sf)
library(tmaptools)
library(fpc)
library(dbscan)
library(ggplot2)
library(OpenStreetMap)
```

```{r Loading & Checking London Bouroughs}
#First, get the London Borough Boundaries
LondonBoroughs <- st_read(here::here("Prac6_data", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))

BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

qtm(BoroughMap)
```

```{r Loading Blue Plaques}
# reading in the blue plaques data from web or geojson file
#Now get the location of all Blue Plaques in the City
#BluePlaques <- st_read("https://s3.eu-west-2.amazonaws.com/openplaques/open-plaques-london-2018-04-08.geojson")

BluePlaques <- st_read(here::here("prac6_data",
                                  "open-plaques-london-2018-04-08.geojson")) %>%
  st_transform(.,27700)

summary(BluePlaques)
```

```{r Quick check of Loaded Data}
#plot the blue plaques in the city
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaques) +
  tm_dots(col = "blue")
```

```{r Data Cleaning}
# notice above, points falling beyond the boundaries.
# we also need to remove duplicate points which might also cause issues with the analysis
# library(tidyverse)
#library(sf)
BluePlaques <- distinct(BluePlaques)
```

```{r Spatial Subsetting}
# only selecting the points inside london
# the second operator[BM, , ] controls which attributes are kept. We leave it blank to manipulate later with tidyverse
BluePlaquesSub <- BluePlaques[BoroughMap,]

# BluePlaquesSub <- BluePlaques[BoroughMap, , op = st_within] #this sets the operator to those within the boundary. The default op is st_intersects which was set in th line above
# others include: st_overlaps, st_touches, st_contains, st_disjoint

#check to see that they've been removed
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```

```{r}
# consider Topological relations between vector geometries in section 6.5.2
#  this function provides the indices of where BoroughMap & BluePlaques intersect.


# add sparse=false to get the complete matrix.
intersect_indices <-st_intersects(BoroughMap, BluePlaques)

# also consider Spatial Clipping section 6.5.3
```

```{r Spatial Joining}
# addressed in Prac5
```

### Key Advice
Question….do i want to:

Select points or polygons in a polygon = Selecting data by location = **spatial sub-setting**

Determine where datasets overlap (or touch, or don’t overlap) and extract those parts = **spatial clipping**

Join two spatial datasets together = **spatial joining**, which can use spatial subsetting functions as the default is `st_intersects()`. This function joins spatial data.

### Study Area Selection

```{r Study Area Selection}
# we will extract one borough to simplify the analysis
#extract the borough of Harrow

# select by attribute
Harrow <- BoroughMap %>%
  filter(., NAME=="Harrow")

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5)
```
### Spatial Clipping 2
```{r Spatial Clipping 2}
#subset the data to our single borough
BluePlaquesSub <- BluePlaques[Harrow,]
#check that it's worked
tmap_mode("plot") # tmap mode set to plotting
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```

```{r Setting Observation Window}
# Setting an observation window for spatstat to carry out the analysis
#now set a window as the borough boundary
window <- as.owin(Harrow)
plot(window)
```

```{r PPP Creation}
# spatstat does not work with sp or sf objects
# it only works with ppp objects, lets create one
# ppp is Planar Point Pattern Object
#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')
#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)

```

```{r}
# lets have a look at the new ppp object
BluePlaquesSub.ppp %>%
  plot(.,pch=16,cex=0.5, #sets plotting symbol as solid circle & cex adjusts the size
       main="Blue Plaques Harrow") # title
```

## 2 Point pattern analysis
### 2.1 Kernel Density Estimation
One way of summarising point data  is by plotting the density of your points under a window called a ‘Kernel’. The size and shape of the Kernel affects the density pattern produced, but it is very easy to produce a Kernel Density Estimation (KDE) map from a ppp object using the density() function.
```{r}
# size and shape of the Kernel affects the density pattern produced but it is
# easy to produce a Kernel Density Estimation (KDE) map from a ppp object using the density() function
BluePlaquesSub.ppp %>%
  density(., sigma=500) %>% # sigma value sets the diameter of the Kernel
  plot()
```

## 2.2 Quadrat Analysis
We are interested in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR. 
The most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the `quadrat count` function in `spatstat`. Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…
```{r}
# ‘complete spatial randomness’ — CSR most basic test is the quadrat analysis
#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6)%>%
    plot(., add=T, col="red")
```

```{r}
# lets save the results into a table
#run the quadrat count
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)

# check the data type in the first column
Qcount %>% 
  summarise_all(class)
```

OK, so we now have a frequency table — next we need to calculate our expected values. The formula for calculating expected probabilities based on the Poisson distribution: 
Poisson Distribution formula Pr... where:

x = var1 is the number of occurrences

λ = lamda is the mean number of occurrences

e is a constant- 2.718
```{r}
# Poisson Distribution
sums <- Qcount %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum))%>%
  dplyr::select(-Var1) 

lambda<- Qcount%>%
  #calculate lambda
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%
  dplyr::select(lambda)%>%
  pull(lambda)
```

Calculate expected probability `Pr` using the Poisson formula from above, `k` is the number of blue plaques counted in a square and is found in the first column of our table…
```{r}
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0))) # sums$Freqquadratcount is number of squares then we round to the nearest integer

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n", # specifying the x and y axis ranges, type indicates no points
xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
points(QCountTable$Freqquadratcount, # adds to the plot above
       col="Red", 
       type="o", # overplotted
       lwd=3) # line width
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)
```

To check for sure, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).

A Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.

If the p-value of our Chi-Squared test is < 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p > 0.05. If our p-value is > 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is < 0.05, this indicates that we do have clustering in our points.

```{r}
teststats <- quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6)
```

```{r}
plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaques in Harrow")
plot(teststats, add=T, col = "red")
```

In the new plot, we can see three figures for each quadrant. The top-left figure is the observed count of points; the top-right is the Poisson expected number of points; the bottom value is the residual value (also known as Pearson residual value), or (Observed - Expected) / Sqrt(Expected).

### 2.3 Experimenting ... 
Try running a quadrant analysis for different grid arrangements (2 x 2, 3 x 3, 10 x 10 etc.) — how does this affect your results?

### 2.4 Ripley’s K
```{r}
K <- BluePlaquesSub.ppp %>%
  Kest(., correction="border") %>%
  plot()
```

```{r}
Kval <- as.data.frame(Kest(BluePlaquesSub.ppp, correction = "Ripley"))
```

## 3 Density-based spatial clustering of applications with noise: DBSCAN
```{r}
#library(fpc)
#first check the coordinate reference system of the Harrow spatial polygon:
st_geometry(BoroughMap)
```

```{r}
#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.)%>%
  as.data.frame()

#now run the dbscan analysis
# Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)
```

```{r}
# used to find suitable eps value based on the knee in plot
# k is no of nearest neighbours used, use min points
#library(dbscan)

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)
```

```{r}
# library(ggplot2)
db
db$cluster
```

```{r}
# We can now add this cluster membership info back into our dataframe
BluePlaquesSubPoints<- BluePlaquesSubPoints %>%
  mutate(dbcluster=db$cluster)
```

```{r}
# Next we are going to create some convex hull polygons to wrap around the points in our clusters
chulls <- BluePlaquesSubPoints %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
  hull = factor(hull, chull(coords.x1, coords.x2)))%>%
  arrange(hull)

#chulls2 <- ddply(BluePlaquesSubPoints, .(dbcluster), 
              #  function(df) df[chull(df$coords.x1, df$coords.x2), ])

# As 0 isn’t actually a cluster (it’s all points that aren’t in a cluster) drop it from the dataframe
chulls <- chulls %>%
  filter(dbcluster >=1)
```

```{r}
# now create a ggplot2 object
dbplot <- ggplot(data=BluePlaquesSubPoints, 
                 aes(coords.x1,coords.x2, colour=dbcluster, fill=dbcluster)) 
#add the points in
dbplot <- dbplot + geom_point()
#now the convex hulls
dbplot <- dbplot + geom_polygon(data = chulls, 
                                aes(coords.x1,coords.x2, group=dbcluster), 
                                alpha = 0.5) 
#now plot, setting the coordinates to scale correctly and as a black and white plot 
#(just for the hell of it)...
dbplot + theme_bw() + coord_equal()
```

```{r}
###add a basemap
##First get the bbox in lat long for Harrow
HarrowWGSbb <- Harrow %>%
  st_transform(., 4326)%>%
  st_bbox()

```

```{r}
#library(OpenStreetMap)

basemap <- OpenStreetMap::openmap(c(51.5549876,-0.4040502),c(51.6405356,-0.2671315),
                         zoom=NULL,
                         "osm")

  # convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +nadgrids=OSTN15_NTv2_OSGBtoETRS.gsb +units=m +no_defs +type=crs")
```

```{r}
#autoplot(basemap_bng) sometimes works
autoplot.OpenStreetMap(basemap_bng)+ 
  geom_point(data=BluePlaquesSubPoints, 
             aes(coords.x1,coords.x2, 
                 colour=dbcluster, 
                 fill=dbcluster)) + 
  geom_polygon(data = chulls, 
               aes(coords.x1,coords.x2, 
                   group=dbcluster,
                   fill=dbcluster), 
               alpha = 0.5)  
```

